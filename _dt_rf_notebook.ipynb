{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project - Inappropriate Language Classification\n",
    "\n",
    "Classification using 3 different models:\n",
    "- Decision tree model -> sourced from the sklearn.tree module. (This notebook)\n",
    "- Random forest model -> sourced from the sklearn.ensemble module. (This notebook)\n",
    "- LSTM model -> sourced from tanserflow.keras module. (LSTM notebook)\n",
    "\n",
    "There are two options for tockenisation and embedding:\n",
    "- **CountVectorizer** (tockenisation)\n",
    "- **GloVe model** (tockenisation + embedding)\n",
    "\n",
    "The **CountVectorizer** has it's dictionary created on the entire dataset, as such, it is limited to the words that are present in the dataset. This might contain words that haven't been seen by pretrained models. It is however entirely sure that pretrained models have seen certain words that aren't present in the dataset. Furethermore it doesn't embed the words, as such the models will need to infer their own relations between then words.\n",
    "\n",
    "The **GloVe model** is a pretrained model trained on a large corpus of words. It allows for both vectorization and embedding of the words. This allows for a first relation between different words to be created. For example, the embeddings of King - Man + Woman would equal Queen. Furthermore, it's dicitonary contains words that are not present in the dataset, this allows the entire trained model to be applied to be applied to new elements containing unseen words more effectively. For example, the word *biatch* not contained in the corpus would have it's embedding close to that of *bitch* which would allow the model to infer it's meaning.\n",
    "\n",
    "This Jupyter Notebook contains the following features:\n",
    "1. Data Preparation (Loading + Tockenisation + Embedding)\n",
    "- Using the CountVectorizer\n",
    "- Using the GloVe model\n",
    "2. Model choice\n",
    "    1. Using a Decision Tree model\n",
    "        - Training\n",
    "        - Testing\n",
    "    2. Using a Random Forest model\n",
    "        - Training\n",
    "        - Testing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "Here we will load the data, choose a tockeniser and preprocess the data by tockenising it. **Choose only one of the tockenisers**, rerunning a block will overwrite the other.\n",
    "\n",
    "### Load Data - Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_baseplate import get_split_count_vectorizer\n",
    "\n",
    "X_train, y_train, X_validate, y_validate, X_test, y_test = get_split_count_vectorizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data - GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "If needed download weights\n",
    "'''\n",
    "from experiment_baseplate import get_glove_model\n",
    "\n",
    "get_glove_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe model\n",
      "Done loading GloVe model\n",
      "\n",
      "Embedding data\n",
      "Done Embedding data\n"
     ]
    }
   ],
   "source": [
    "from experiment_baseplate import get_split_glove_embedding\n",
    "\n",
    "X_train, y_train, X_validate, y_validate, X_test, y_test = get_split_glove_embedding()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model selection\n",
    "\n",
    "### 1. Decision Tree\n",
    "\n",
    "The decision tree model is built and trained by sklearn. Parameters are specified by the library.\n",
    "\n",
    "#### Training\n",
    "\n",
    "Make sure you have imported the data, click on run, sit back and relax while the model trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training finished...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_t = DecisionTreeClassifier()\n",
    "\n",
    "print(\"Training...\")\n",
    "clf_t.fit(X_train, y_train)\n",
    "print(\"Training finished...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "\n",
    "Once you have trained, click on run and get the results on unseen data. You will have both test results on the validate and test. That is because the validation dataset wasn't used during training and it is bigger. The testing dataset is good to compare with the other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate values -> accuracy : 0.9463639765498316 | precision : 0.9473760721035034 | recall : 0.9290092658588739\n",
      "Test values -> accuracy : 0.9433703380316827 | precision : 0.9443392102579047 | recall : 0.9249322106464963\n"
     ]
    }
   ],
   "source": [
    "from experiment_baseplate import score\n",
    "\n",
    "print(\"Decision Tree Model\")\n",
    "print(\"Validate values -> \" + score( clf_t.predict(X_validate) , y_validate))\n",
    "print(\"Test values -> \" + score( clf_t.predict(X_test) , y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Forest\n",
    "\n",
    "The decision tree model is built and trained by sklearn. Parameters are specified by the library.\n",
    "\n",
    "#### Training\n",
    "\n",
    "Make sure you have imported the data, click on run, sit back and relax while the model trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training finished...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_rf = RandomForestClassifier()\n",
    "\n",
    "print(\"Training...\")\n",
    "clf_rf.fit(X_train, y_train)\n",
    "print(\"Training finished...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "\n",
    "Once you have trained, click on run and get the results on unseen data. You will have both test results on the validate and test. That is because the validation dataset wasn't used during training and it is bigger. The testing dataset is good to compare with the other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model\n",
      "Validate values -> accuracy : 0.9476736934015217 | precision : 0.9643609022556391 | recall : 0.914183891660727\n",
      "Test values -> accuracy : 0.9487963078458276 | precision : 0.9660940325497287 | recall : 0.9149422006564863\n"
     ]
    }
   ],
   "source": [
    "from experiment_baseplate import score\n",
    "\n",
    "print(\"Random Forest Model\")\n",
    "print(\"Validate values -> \" + score( clf_rf.predict(X_validate) , y_validate))\n",
    "print(\"Test values -> \" + score( clf_rf.predict(X_test) , y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bad_lang_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
