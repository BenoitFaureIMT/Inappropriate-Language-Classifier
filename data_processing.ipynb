{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "This notebook processes raw/downloaded data and compiles it into our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/vscode/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(columns=[\"text\", \"class\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. HSAOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsaol = pd.read_csv(\"downloads/hsaol.csv\", sep=\",\")[[\"tweet\",\"class\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_426/2866594782.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  hsaol[\"tweet\"] = hsaol[\"tweet\"].str.replace(\"^!*\", \"\")\n",
      "/tmp/ipykernel_426/2866594782.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  hsaol[\"tweet\"] = hsaol[\"tweet\"].str.replace(\"\\sRT\\s@.*:\", \" \")\n"
     ]
    }
   ],
   "source": [
    "hsaol[\"tweet\"] = hsaol[\"tweet\"].str.replace(\"^!*\", \"\")\n",
    "hsaol[\"tweet\"] = hsaol[\"tweet\"].str.replace(\"\\sRT\\s@.*:\", \" \")\n",
    "hsaol[\"inappro\"] = (hsaol[\"class\"] < 2).astype(int)\n",
    "\n",
    "hsaol = hsaol[[\"tweet\", \"inappro\"]].rename(columns={'tweet': 'text', 'inappro': 'class'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([dataset, hsaol], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Measuring Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "measuringhatespeech = pd.read_csv(\"downloads/measuring_hate_speech.csv\", sep=\",\")[[\"text\", \"hate_speech_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "measuringhatespeech[\"inappro\"] = (measuringhatespeech[\"hate_speech_score\"] > 0.5).astype(int)\n",
    "\n",
    "measuringhatespeech = measuringhatespeech[[\"text\", \"inappro\"]].rename(columns={'inappro': 'class'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([dataset, measuringhatespeech], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Insults Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "insults_test = pd.read_csv(\"downloads/insults/test_with_solutions.csv\", sep=\",\")[[\"Comment\", \"Insult\"]]\n",
    "insults_train = pd.read_csv(\"downloads/insults/train.csv\", sep=\",\")[[\"Comment\", \"Insult\"]]\n",
    "insults = pd.concat([insults_train, insults_test], ignore_index=True)\n",
    "\n",
    "insults = insults.rename(columns={\"Comment\": \"text\", \"Insult\": \"class\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([dataset, insults], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Jigsaw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 153164 entries, 0 to 153163\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             153164 non-null  object\n",
      " 1   comment_text   153164 non-null  object\n",
      " 2   toxic          153164 non-null  int64 \n",
      " 3   severe_toxic   153164 non-null  int64 \n",
      " 4   obscene        153164 non-null  int64 \n",
      " 5   threat         153164 non-null  int64 \n",
      " 6   insult         153164 non-null  int64 \n",
      " 7   identity_hate  153164 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 10.5+ MB\n"
     ]
    }
   ],
   "source": [
    "jigsaw_test_labels = pd.read_csv(\"downloads/jigsaw/test_labels.csv\", sep=\",\")\n",
    "jigsaw_test = pd.read_csv(\"downloads/jigsaw/test.csv\", sep=',')\n",
    "jigsaw_test = jigsaw_test.merge(jigsaw_test_labels, on=\"id\")\n",
    "jigsaw_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 312735 entries, 0 to 312734\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   comment_text   312735 non-null  object\n",
      " 1   toxic          312735 non-null  int64 \n",
      " 2   severe_toxic   312735 non-null  int64 \n",
      " 3   obscene        312735 non-null  int64 \n",
      " 4   threat         312735 non-null  int64 \n",
      " 5   insult         312735 non-null  int64 \n",
      " 6   identity_hate  312735 non-null  int64 \n",
      " 7   id             153164 non-null  object\n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 19.1+ MB\n"
     ]
    }
   ],
   "source": [
    "jigsaw_train = pd.read_csv(\"downloads/jigsaw/train.csv\", sep=\",\")[[\"comment_text\",\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]]\n",
    "jigsaw = pd.concat([jigsaw_train, jigsaw_test], ignore_index=True)\n",
    "jigsaw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "jigsaw[\"class\"] = jigsaw.apply(lambda x: 1 if x['toxic'] == 1 or x['severe_toxic'] == 1 or x['obscene'] == 1 or x['threat'] == 1 or x['insult'] == 1 or x['identity_hate'] == 1 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jigsaw = jigsaw[[\"comment_text\", \"class\"]].rename(columns={\"comment_text\": \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jigsaw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([dataset, jigsaw], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Jibes and Delight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructJibesFile(data_type):\n",
    "    data = {\"text\": [], \"class\": []}\n",
    "    for i in [0, 1]:\n",
    "        with open(f\"downloads/jibesanddelights/comment.{data_type}.{i}.txt\") as f:\n",
    "            for line in f.readlines():\n",
    "                data[\"text\"].append(line)\n",
    "                data[\"class\"].append(i)\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "jad_dev = constructJibesFile(\"dev\")\n",
    "jad_test = constructJibesFile(\"test\")\n",
    "jad_train = constructJibesFile(\"train\")\n",
    "\n",
    "jad = pd.concat([jad_dev, jad_test, jad_train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([dataset, jad], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text: str):\n",
    "    text = text.lower().removeprefix(\"\\\"\").removesuffix(\"\\\"\")\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"cleaned\"] = dataset.apply(lambda x : clean_text(x[\"text\"]), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text:\t598929\n",
      "Class 0:\t453949\t\t(75%)\n",
      "Class 1:\t144980\t\t(24%)\n"
     ]
    }
   ],
   "source": [
    "#Display dataset info\n",
    "total = len(dataset)\n",
    "app = len(dataset[(dataset[\"class\"] == 0)])\n",
    "inapp = len(dataset[(dataset[\"class\"] == 1)])\n",
    "print(f\"Total text:\\t{total}\\nClass 0:\\t{app}\\t\\t({int(app / total * 100)}%)\\nClass 1:\\t{inapp}\\t\\t({int(inapp / total * 100)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 419250 69%\n",
      "Validate: 89839 14%\n",
      "Test: 89840 15%\n"
     ]
    }
   ],
   "source": [
    "# Split Dataset\n",
    "\n",
    "train, validate, test = np.split(dataset.sample(frac=1), [int(.7*len(dataset)), int(.85*len(dataset))])\n",
    "\n",
    "\n",
    "print(f\"Train: {len(train)} {int(len(train)/len(dataset)*100)}%\")\n",
    "print(f\"Validate: {len(validate)} {int(len(validate)/len(dataset)*100)}%\")\n",
    "print(f\"Test: {len(test)} {int(len(test)/len(dataset)*100)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Dataset\n",
    "train.to_csv(\"data/cleaned/train.csv\")\n",
    "validate.to_csv(\"data/cleaned/validate.csv\")\n",
    "test.to_csv(\"data/cleaned/test.csv\")\n",
    "dataset.to_csv(\"data/cleaned/full.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
