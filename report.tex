\documentclass[11pt,a4paper]{article}
\usepackage[english]{babel}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{eso-pic}
\pagestyle{fancy}
\graphicspath{ {./images/} }

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in
\setlength{\headheight}{13.6pt}
\fancyheadoffset{0pt}


\title{%
    \begin{minipage}\linewidth
        \centering\bfseries\sffamily
        Machine Learning Project
        \vskip 3pt
        \large Classification of inappropriate language in game chats
    \end{minipage}
}

\AddToShipoutPictureBG{
    \AtPageLowerLeft{
        \hspace{1cm}
        \raisebox{1cm}{\includegraphics[width=2cm]{images/logoEcole.png}}%
    }
}

\renewcommand{\headrulewidth}{1pt}
\fancyhead[R]{\textbf{Classification of inappropriate language in game chats}} 
\fancyhead[L]{\leftmark}
\fancyhead[C]{}


\author{ STORAI Romain, FAURE Benoit, MATHIEU Antoine }
\date{\today}

\begin{document}
\maketitle	

\tableofcontents
\pagebreak

\section{Introduction}

\subsection{The goal of the project}
Our Subject tackle the problem of innapropriate language in video-game chat. For now to detect innapropriate video-games mainly use lists of ban word to try to stop innapropriate language. That is not efficient because ban list do not encapsulate a context. That mean that a innapropriate sentence with no ban word will not be detected by this system. Our idea is to study multiple model  of machine learning to classify innapropriate sentence. The goal is to take into account the context of a sentence to determine if it innapropriate.   

\subsection{What is Inappropriate Language?}
We define, for the following work, an \textit{Inappropriate Language} as a word or a sentence that contains any of the following:\\

\begin{itemize}
  \item \textbf{Profanity} - This includes any language that is considered vulgar, offensive, or obscene. This can include swear words, sexual language, and derogatory terms.
  \item \textbf{Hate speech} - Hate speech is language that is intended to demean, discriminate against, or incite violence or hatred towards a particular group of people based on their race, ethnicity, gender, religion, sexual orientation, or other characteristic.
  \item \textbf{Insults} - This includes any language that is intended to insult or belittle someone else. This can include name-calling, personal attacks, or derogatory comments about someone's appearance, abilities, or personality.
  \item \textbf{Threats} Threats are language that is intended to intimidate or harm another person. This can include physical threats, verbal abuse, or intimidation.
\end{itemize}







\pagebreak
\section{Benchmarking}
Define the methodology and the protocol of the experiment. Also describe shortly how we did it\\

For our study we decided to  look at the problem as a problem of binary classification. We have two classes for a sentence \textit{(Appropriate/Innapropriate)}. We have also restricted our study to the case of supervised learning. We find different dataset of innapropriate sentences that are labelised. We preprocess our data to clean it and create only two classes to fit out binary classification.


\subsection{Dataset}

\subsubsection{Details}

Our dataset contains texts associated with a class. The class can be Appropriate (0) or Inappropriate (1).\\
The texts are in English and come from social media comments (Twitter, Reddit, Youtube and Wikipedia Comments). We are aware that this dataset will not be able to fully answer our problem given its source: video game chat texts are very different from social medias comments. However, we have not been able to find such data in the gaming domain.\\
We will describe bellow the creation of the dataset, but here is the global decriptive analysis:\\
Include analysis.

\subsubsection{Creating the dataset}
Our dataset is a merge of multiple datasets. For each of the models, we have selected characteristics that allow us to categorize the texts as appropriate or not. Moreover, we tried to vary types of inappropriate language: there are datasets of Hate Speech that target groups of individuals, as well as datasets of insults that target individuals directly. This will ensure that we do not bias our model with certain types of inappropriate language.\\
POURCENTAGE OF HS AND INSULTS\\
After gathering and merging the data, we cleaned the data to reduce the complexity of it:
\begin{itemize}
    \item We lowercased all the characters,
    \item Removed unecessary English stop words,
    \item Tried to remove as much as non-english sentences as possible,
    \item And lemmatized all words, which is the process of reducing a word to its base or dictionary form (lemma form).
\end{itemize}

\subsubsection{Sources}

And how we merged them.

\subsection{Models}

\subsubsection{Preamble}
The entry date of our model  is  words or sentences that means that we have to do some NLP to transforms our text and make it understable for a machine and our model. For that we used a tokenizer and an embbeder. The Tokenizer splits the text into smaller units such as words or characters to make it easier to manipulate.Next, the text is transformed into numerical vectors using an embedder. An embedder maps the text into a high-dimensional vector space where each dimension represents a unique feature of the text.\\
In the first time we used a classic tokenizer and embbder then with used a pre-trained embbeder called GloVE train en tweets to see if we could get more efficient.

We have to make a short review of NLP and explain shortly how ML and DL deal with text (tokenizer, embedder).\\Then we'll add that we also used pretrained embedders - GLoVE. 

\subsubsection{Decision Tree}

\subsubsection{Random Forest}

\subsubsection{Support Vector Classifier}

\subsubsection{Long Short Term Mermory (LSTM)}

\subsubsection{Transformers (DistilBERT)?}

\subsection{Metrics}
In this section we will introduce and defined the metrics that we are going to use to validate our model. 
First our problem is a binary classification problem that mean that we have only two outcome true or false (In our case inappropriate or not). 
So, after prediction we only have 4 cases.
\includegraphics{images/ConfusionMatrix.png}

\subsubsection{Accuracy}
The first metric that we are going to use is called Accuracy: It’s defined by the number correct prediction (TP and TN) divided by all the prediction.

\begin{equation}
Accuracy =  \frac{TP+TN}{TP+TN+FP+FN}
\end{equation}
But in our case our dataset is unbalanced (70 appropriate/30innapropriate) that mean that our accuracy doesn’t tell us if, when we have an inappropriate language, we predict really has an inappropriate language. That’s why we need to search for other metrics to validate out model.

\subsubsection{Precision and Recall}

To evaluate our model we decided to use the precision. The Precision is defined by the number of correct positive prediction divided by all the positive prediction. In our case that mean that for all the inappropriate prediction we calculate the proportion of correct one.
\begin{equation}
Precision = \frac {TP}{TP+FP}
\end{equation}

Another metric is needed to reinforce our first metric. It’s the recall defined by the number of correct positive prediction divided by all the positive elements. That mean that for all the inappropriate language we calculated the proportion of correct prediction by our model. 
\begin{equation}
Recall = \frac {TP}{TP+FN}
\end{equation}
We’ve these two new metrics we can evaluate our model and see if it’s efficient to predict correctly inappropriate language.

\subsubsection{F-1 Score}
Now we’ve these 3 metrics we can introduce the F1 score. F1 score is a metric to calculate a model’s accuracy but it’s considering the precision and recall score. The F1 score combines precision and recall using their harmonic mean, and maximizing the F1 score implies simultaneously maximizing both precision and recall. For us that mean to maximize the efficience of our model to correctly predict innapropriate language.

\begin{equation}
F1 = \frac{2*Precision*Recall} {Precision + Recall} = \frac{2*TP}{2*TP+FP+FN}
\end{equation}

\subsubsection{Time inference}
The last metric that we want to use is called Time inference. The time inference refers to the time it takes to the model to make a prediction. In our subject it’s really important because our goal is to detect inappropriate language in in game-chat. That mean that when a player writes a message we need to detect rapidly if it’s inappropriate to supress the message.\\ 
We’ve those five metrics we can  now precisely evaluate our models.






 



\pagebreak
\section{Results}
Make tables with all models and metrics.\\
Add also how it performed with "new data"

\pagebreak
\section{Final Model}
Motivate the selection of your final solution.\\
Deliver that solution in the form of a predictive model that can be used on new input
data.

\end{document}